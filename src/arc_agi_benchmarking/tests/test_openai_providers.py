import pytest
from unittest.mock import Mock, MagicMock, patch
import inspect # Added for subclass discovery
import sys # Added for subclass discovery

from arc_agi_benchmarking.adapters.openai_base import OpenAIBaseAdapter
from arc_agi_benchmarking.adapters.open_ai import OpenAIAdapter
from arc_agi_benchmarking.adapters.grok import GrokAdapter
from arc_agi_benchmarking.adapters.deepseek import DeepseekAdapter
from arc_agi_benchmarking.adapters.fireworks import FireworksAdapter
from arc_agi_benchmarking.adapters.openrouter import OpenRouterAdapter
# Import all adapters to ensure they are available for discovery
import arc_agi_benchmarking.adapters 

from arc_agi_benchmarking.schemas import ModelConfig, ModelPricing, Usage, CompletionTokensDetails, Cost, APIType
from openai.types import CompletionUsage
from arc_agi_benchmarking.errors import TokenMismatchError
import os
from dotenv import load_dotenv

load_dotenv() # Load environment variables for potential API key checks during init

# --- Helper Function for Subclass Discovery ---
def find_concrete_subclasses(cls):
    """Finds all concrete subclasses of a given class currently imported."""
    concrete_subclasses = []
    for name, obj in inspect.getmembers(sys.modules[cls.__module__.split('.')[0]]): # Look within the 'arc_agi_benchmarking' module tree
        if inspect.ismodule(obj):
             for sub_name, sub_obj in inspect.getmembers(obj):
                  if inspect.isclass(sub_obj) and \
                     issubclass(sub_obj, cls) and \
                     sub_obj != cls and \
                     not inspect.isabstract(sub_obj):
                      # Check specifically within adapters submodule
                      if 'adapters' in sub_obj.__module__:
                           concrete_subclasses.append(sub_obj)
                           
    # Need to also check adapters imported directly
    for name, obj in inspect.getmembers(arc_agi_benchmarking.adapters):
         if inspect.isclass(obj) and issubclass(obj, cls) and obj != cls and not inspect.isabstract(obj):
             if obj not in concrete_subclasses:
                  concrete_subclasses.append(obj)
                  
    return concrete_subclasses

# Dynamically find all concrete subclasses of OpenAIBaseAdapter
openai_adapter_subclasses = find_concrete_subclasses(OpenAIBaseAdapter)

# --- Fixtures ---

@pytest.fixture
def mock_model_config(adapter_class):
    """Provides a mock ModelConfig, adapting provider based on the adapter class."""
    # Infer provider name from class name (e.g., OpenAIAdapter -> openai)
    provider_name = adapter_class.__name__.lower().replace("adapter", "")
    config_name = f"test-{provider_name}-model"
    model_name = f"{provider_name}-test-model"
    
    # Handle potential variations if needed (e.g., fireworks vs fireworks-ai)
    # For now, simple lowercase name works for openai, grok, deepseek, fireworks

    return ModelConfig(
        name=config_name,
        model_name=model_name,
        provider=provider_name, 
        pricing=ModelPricing(date="2024-01-01", input=1.0, output=2.0), 
        api_type=APIType.CHAT_COMPLETIONS, # Assuming chat completions for simplicity
        kwargs={"temperature": 0.5}
    )

# --- Fixtures for Different Cost Calculation Scenarios ---

@pytest.fixture
def mock_response_case_a_no_reasoning():
    """Case A: pt + ct == tt. No explicit reasoning provided."""
    mock_response = Mock()
    mock_response.usage = Mock()
    mock_response.usage.prompt_tokens = 100
    mock_response.usage.completion_tokens = 200 # ct_raw
    mock_response.usage.total_tokens = 300      # tt_raw
    # Simulate provider not giving reasoning details or details object
    mock_response.usage.completion_tokens_details = Mock()
    mock_response.usage.completion_tokens_details.reasoning_tokens = 0 # rt_explicit = 0
    return mock_response

@pytest.fixture
def mock_response_case_a_explicit_reasoning():
    """Case A: pt + ct == tt. Explicit reasoning provided."""
    mock_response = Mock()
    mock_response.usage = Mock()
    mock_response.usage.prompt_tokens = 100
    mock_response.usage.completion_tokens = 220 # ct_raw (includes the 20 reasoning)
    mock_response.usage.total_tokens = 320      # tt_raw
    mock_response.usage.completion_tokens_details = Mock()
    mock_response.usage.completion_tokens_details.reasoning_tokens = 20 # rt_explicit = 20
    return mock_response
    
@pytest.fixture
def mock_response_case_b_explicit_reasoning():
    """Case B: pt + ct < tt. Explicit reasoning provided."""
    mock_response = Mock()
    mock_response.usage = Mock()
    mock_response.usage.prompt_tokens = 50
    mock_response.usage.completion_tokens = 150 # ct_raw (separate from reasoning)
    mock_response.usage.total_tokens = 210      # tt_raw (implies 10 reasoning, matches explicit)
    mock_response.usage.completion_tokens_details = Mock()
    mock_response.usage.completion_tokens_details.reasoning_tokens = 10 # rt_explicit = 10
    return mock_response

@pytest.fixture
def mock_response_case_b_inferred_reasoning():
    """Case B: pt + ct < tt. Reasoning must be inferred."""
    mock_response = Mock()
    mock_response.usage = Mock()
    mock_response.usage.prompt_tokens = 100
    mock_response.usage.completion_tokens = 200 # ct_raw (separate from reasoning)
    mock_response.usage.total_tokens = 320      # tt_raw (implies 20 reasoning)
    # Simulate provider not giving details or reasoning_tokens=0
    mock_response.usage.completion_tokens_details = Mock()
    mock_response.usage.completion_tokens_details.reasoning_tokens = 0 # rt_explicit = 0
    return mock_response

@pytest.fixture
def mock_openai_response_mismatch():
    """Provides a mock OpenAI API response with mismatched token counts that cannot be reconciled."""
    mock_response = Mock()
    mock_response.usage = Mock()
    mock_response.usage.prompt_tokens = 100
    mock_response.usage.completion_tokens = 200
    mock_response.usage.total_tokens = 350  # Provider claims 350
    # Explicitly claims some reasoning tokens, but not enough to reconcile
    mock_response.usage.completion_tokens_details = Mock()
    # Even with explicit rt=10, Case B logic applies (pt+ct < tt). 
    # computed_total = pt(100) + ct(200) + rt(10) = 310. Mismatch with tt(350).
    mock_response.usage.completion_tokens_details.reasoning_tokens = 10  
    return mock_response
    

# --- Test Class ---
# Parameterize the whole class to run tests for each adapter subclass
@pytest.mark.parametrize("adapter_class", openai_adapter_subclasses)
class TestOpenAIBaseProviderLogic:

    @pytest.fixture(autouse=True)
    def patch_env_vars(self, monkeypatch):
        """Ensure necessary API keys are mocked for adapter initialization."""
        monkeypatch.setenv("OPENAI_API_KEY", "test-key")
        monkeypatch.setenv("XAI_API_KEY", "test-key")
        monkeypatch.setenv("DEEPSEEK_API_KEY", "test-key")
        monkeypatch.setenv("FIREWORKS_API_KEY", "test-key")

    @pytest.fixture
    def adapter_instance(self, adapter_class, mock_model_config):
        """Provides an instance of the parameterized adapter class, bypassing real init."""
        
        # Patch the base ProviderAdapter.__init__ to prevent it from running
        # We will manually set the necessary attributes (model_config, client)
        with patch("arc_agi_benchmarking.adapters.provider.ProviderAdapter.__init__", return_value=None) as mock_provider_init:
            
            # Instantiate the parameterized adapter class (init is mocked)
            adapter = adapter_class(config=mock_model_config.name) 
            
            # Manually set the attributes that ProviderAdapter.__init__ would normally set
            adapter.config = mock_model_config.name
            adapter.model_config = mock_model_config 
            adapter.client = MagicMock() # Mock the client instance directly
            
            return adapter

    # --- Test _get_usage (Ensures input to cost calculation is correct) ---
    def test_get_usage_case_a_no_reasoning(self, adapter_instance, mock_response_case_a_no_reasoning):
        """Test _get_usage extracts tokens correctly for Case A, no explicit reasoning."""
        usage = adapter_instance._get_usage(mock_response_case_a_no_reasoning)
        assert isinstance(usage, Usage)
        assert usage.prompt_tokens == 100
        assert usage.completion_tokens == 200 # raw ct
        assert usage.total_tokens == 300
        assert usage.completion_tokens_details.reasoning_tokens == 0 # rt_explicit

    def test_get_usage_case_a_explicit_reasoning(self, adapter_instance, mock_response_case_a_explicit_reasoning):
        """Test _get_usage extracts tokens correctly for Case A, explicit reasoning."""
        usage = adapter_instance._get_usage(mock_response_case_a_explicit_reasoning)
        assert usage.prompt_tokens == 100
        assert usage.completion_tokens == 220 # raw ct
        assert usage.total_tokens == 320
        assert usage.completion_tokens_details.reasoning_tokens == 20 # rt_explicit

    def test_get_usage_case_b_explicit_reasoning(self, adapter_instance, mock_response_case_b_explicit_reasoning):
        """Test _get_usage extracts tokens correctly for Case B, explicit reasoning."""
        usage = adapter_instance._get_usage(mock_response_case_b_explicit_reasoning)
        assert usage.prompt_tokens == 50
        assert usage.completion_tokens == 150 # raw ct
        assert usage.total_tokens == 210
        assert usage.completion_tokens_details.reasoning_tokens == 10 # rt_explicit

    def test_get_usage_case_b_inferred_reasoning(self, adapter_instance, mock_response_case_b_inferred_reasoning):
        """Test _get_usage extracts tokens and infers reasoning correctly for Case B."""
        usage = adapter_instance._get_usage(mock_response_case_b_inferred_reasoning)
        assert usage.prompt_tokens == 100
        assert usage.completion_tokens == 200 # raw ct
        assert usage.total_tokens == 320
        # IMPORTANT: _get_usage itself infers reasoning here when rt_explicit=0 and pt+ct < tt
        assert usage.completion_tokens_details.reasoning_tokens == 20 # inferred rt

    # --- Test _calculate_cost for different cases ---
    def test_calculate_cost_case_a_no_reasoning(self, adapter_instance, mock_response_case_a_no_reasoning):
        """Test cost calculation for Case A (pt+ct=tt), no explicit reasoning."""
        cost = adapter_instance._calculate_cost(mock_response_case_a_no_reasoning)
        # pt=100, ct_raw=200, rt_explicit=0
        # -> ct_for_cost = 200, rt_for_cost = 0
        expected_prompt_cost = 100 * (1.0 / 1_000_000)
        expected_completion_cost = 200 * (2.0 / 1_000_000) # Based on ct_for_cost
        expected_reasoning_cost = 0 * (2.0 / 1_000_000)    # Based on rt_for_cost
        expected_total_cost = expected_prompt_cost + expected_completion_cost + expected_reasoning_cost
        
        assert isinstance(cost, Cost)
        assert cost.prompt_cost == pytest.approx(expected_prompt_cost)
        assert cost.completion_cost == pytest.approx(expected_completion_cost)
        assert cost.reasoning_cost == pytest.approx(expected_reasoning_cost)
        assert cost.total_cost == pytest.approx(expected_total_cost)

    def test_calculate_cost_case_a_explicit_reasoning(self, adapter_instance, mock_response_case_a_explicit_reasoning):
        """Test cost calculation for Case A (pt+ct=tt), explicit reasoning."""
        cost = adapter_instance._calculate_cost(mock_response_case_a_explicit_reasoning)
        # pt=100, ct_raw=220, rt_explicit=20
        # -> ct_for_cost = 200 (220-20), rt_for_cost = 20
        expected_prompt_cost = 100 * (1.0 / 1_000_000)
        expected_completion_cost = 200 * (2.0 / 1_000_000) # Based on ct_for_cost
        expected_reasoning_cost = 20 * (2.0 / 1_000_000)   # Based on rt_for_cost
        expected_total_cost = expected_prompt_cost + expected_completion_cost + expected_reasoning_cost

        assert cost.prompt_cost == pytest.approx(expected_prompt_cost)
        assert cost.completion_cost == pytest.approx(expected_completion_cost)
        assert cost.reasoning_cost == pytest.approx(expected_reasoning_cost)
        assert cost.total_cost == pytest.approx(expected_total_cost)
        
    def test_calculate_cost_case_b_explicit_reasoning(self, adapter_instance, mock_response_case_b_explicit_reasoning):
        """Test cost calculation for Case B (pt+ct<tt), explicit reasoning."""
        cost = adapter_instance._calculate_cost(mock_response_case_b_explicit_reasoning)
        # pt=50, ct_raw=150, tt=210, rt_explicit=10
        # -> ct_for_cost = 150 (ct_raw), rt_for_cost = 10 (rt_explicit)
        expected_prompt_cost = 50 * (1.0 / 1_000_000)
        expected_completion_cost = 150 * (2.0 / 1_000_000) # Based on ct_for_cost
        expected_reasoning_cost = 10 * (2.0 / 1_000_000)   # Based on rt_for_cost
        expected_total_cost = expected_prompt_cost + expected_completion_cost + expected_reasoning_cost

        assert cost.prompt_cost == pytest.approx(expected_prompt_cost)
        assert cost.completion_cost == pytest.approx(expected_completion_cost)
        assert cost.reasoning_cost == pytest.approx(expected_reasoning_cost)
        assert cost.total_cost == pytest.approx(expected_total_cost)
        
    def test_calculate_cost_case_b_inferred_reasoning(self, adapter_instance, mock_response_case_b_inferred_reasoning):
        """Test cost calculation for Case B (pt+ct<tt), inferred reasoning."""
        cost = adapter_instance._calculate_cost(mock_response_case_b_inferred_reasoning)
        # pt=100, ct_raw=200, tt=320, rt_explicit=0
        # -> inferred rt = 20 (320 - (100+200))
        # -> ct_for_cost = 200 (ct_raw), rt_for_cost = 20 (inferred)
        expected_prompt_cost = 100 * (1.0 / 1_000_000)
        expected_completion_cost = 200 * (2.0 / 1_000_000) # Based on ct_for_cost
        expected_reasoning_cost = 20 * (2.0 / 1_000_000)   # Based on rt_for_cost (inferred)
        expected_total_cost = expected_prompt_cost + expected_completion_cost + expected_reasoning_cost

        assert cost.prompt_cost == pytest.approx(expected_prompt_cost)
        assert cost.completion_cost == pytest.approx(expected_completion_cost)
        assert cost.reasoning_cost == pytest.approx(expected_reasoning_cost)
        assert cost.total_cost == pytest.approx(expected_total_cost)

    def test_calculate_cost_token_mismatch_error(self, adapter_instance, mock_openai_response_mismatch):
        """Test _calculate_cost raises TokenMismatchError on mismatch."""
        with pytest.raises(TokenMismatchError) as excinfo:
            adapter_instance._calculate_cost(mock_openai_response_mismatch)
        # Check the specific error message format from the latest code
        # pt=100, ct_raw=200, tt=350, rt_explicit=10 -> Case B
        # ct_for_cost=200, rt_for_cost=10, pt=100 -> computed_total=310
        expected_msg_part1 = "Token count mismatch: API reports total 350"
        expected_msg_part2 = "but computed P:100 + C:200 + R:10 = 310"
        assert expected_msg_part1 in str(excinfo.value)
        assert expected_msg_part2 in str(excinfo.value)

    # --- Test make_prediction flow for each subclass --- 
    # Use Case A (no reasoning) for the flow test for simplicity
    def test_make_prediction_flow(self, adapter_class, adapter_instance, mock_response_case_a_no_reasoning):
        """Test the overall flow of make_prediction using Case A (no reasoning) fixture."""
        
        # Patch the methods on the *specific class* being tested
        with patch.object(adapter_class, '_call_ai_model') as mock_call_ai, \
             patch.object(adapter_class, '_get_content', return_value='[[1]]') as mock_get_content, \
             patch.object(adapter_class, '_get_role', return_value="assistant") as mock_get_role, \
             patch.object(adapter_class, '_get_reasoning_summary', return_value="Reasoning summary") as mock_get_reasoning_summary:

            mock_call_ai.return_value = mock_response_case_a_no_reasoning # Use the Case A fixture
            
            prompt = "Test prompt"
            attempt = adapter_instance.make_prediction(prompt)

            mock_call_ai.assert_called_once_with(prompt)
            assert mock_get_content.called
            assert mock_get_role.called
            
            # Check the final attempt object metadata
            assert attempt.answer == [[1]]
            assert attempt.metadata.model == adapter_instance.model_config.model_name
            assert attempt.metadata.provider == adapter_instance.model_config.provider
            assert attempt.metadata.usage.prompt_tokens == 100
            assert attempt.metadata.usage.completion_tokens == 200 # raw ct
            assert attempt.metadata.usage.total_tokens == 300
            assert attempt.metadata.usage.completion_tokens_details.reasoning_tokens == 0 # rt_explicit
            
            # Verify cost calculation based on Case A (no reasoning) logic
            # ct_for_cost = 200, rt_for_cost = 0
            expected_prompt_cost = 100 * (1.0 / 1_000_000)
            expected_completion_cost = 200 * (2.0 / 1_000_000)
            expected_reasoning_cost = 0 * (2.0 / 1_000_000)
            expected_total_cost = expected_prompt_cost + expected_completion_cost + expected_reasoning_cost
            
            assert attempt.metadata.cost.prompt_cost == pytest.approx(expected_prompt_cost)
            assert attempt.metadata.cost.completion_cost == pytest.approx(expected_completion_cost)
            assert attempt.metadata.cost.reasoning_cost == pytest.approx(expected_reasoning_cost)
            assert attempt.metadata.cost.total_cost == pytest.approx(expected_total_cost)
            
            assert attempt.metadata.kwargs == {"temperature": 0.5}
            assert len(attempt.metadata.choices) == 2
            assert attempt.metadata.choices[0].message.role == "user"
            assert attempt.metadata.choices[0].message.content == prompt
            assert attempt.metadata.choices[1].message.role == "assistant"
            assert attempt.metadata.choices[1].message.content == "[[1]]"

    # --- Test streaming functionality ---
    def test_call_ai_model_streaming_enabled(self, adapter_instance, mock_response_case_a_no_reasoning):
        """Test that _call_ai_model uses streaming when stream=True in config."""
        # Set streaming to True in model config
        adapter_instance.model_config.stream = True
        adapter_instance.model_config.api_type = APIType.CHAT_COMPLETIONS
        adapter_instance.model_config.kwargs = {'stream': True}
        
        # Mock the _chat_completion_stream method to check if streaming is used
        with patch.object(adapter_instance, '_chat_completion_stream') as mock_completion_stream:
            mock_completion_stream.return_value = mock_response_case_a_no_reasoning
            
            prompt = "Test streaming prompt"
            
            result = adapter_instance._call_ai_model(prompt)
            
            # Verify _chat_completion_stream was called for streaming
            mock_completion_stream.assert_called_once()
            # Verify the result is our expected response
            assert result == mock_response_case_a_no_reasoning

    def test_call_ai_model_streaming_disabled(self, adapter_instance, mock_response_case_a_no_reasoning):
        """Test that _call_ai_model uses regular completion when stream=False or not set."""
        # Ensure streaming is False or not set
        adapter_instance.model_config.stream = False
        adapter_instance.model_config.api_type = APIType.CHAT_COMPLETIONS
        
        with patch.object(adapter_instance, '_chat_completion') as mock_completion, \
             patch.object(adapter_instance, '_chat_completion_stream') as mock_stream:
            
            mock_completion.return_value = mock_response_case_a_no_reasoning
            
            prompt = "Test non-streaming prompt"
            messages = [{"role": "user", "content": prompt}]
            
            result = adapter_instance._call_ai_model(prompt)
            
            # Verify regular completion was called
            mock_completion.assert_called_once_with(messages)
            # Verify streaming was NOT called
            mock_stream.assert_not_called()
            # Verify the result is our expected response
            assert result == mock_response_case_a_no_reasoning

    def test_chat_completion_stream_functionality(self, adapter_instance, mock_response_case_a_no_reasoning):
        """Test the streaming functionality works correctly with proper chunk handling."""
        # Create mock chunks for streaming
        mock_chunks = []
        for i in range(3):
            chunk = MagicMock()
            chunk.choices = [MagicMock()]
            chunk.choices[0].delta.content = f"chunk{i}"
            chunk.choices[0].finish_reason = "stop" if i == 2 else None  # Only set finish_reason on last chunk
            chunk.id = "test-id"
            chunk.created = 12345
            chunk.model = "test-model"
            # Mock usage data on the last chunk
            if i == 2:
                chunk.usage = CompletionUsage(
                    prompt_tokens=10,
                    completion_tokens=20,
                    total_tokens=30
                )
            mock_chunks.append(chunk)
        
        # Mock the client's chat.completions.create to return an iterable
        adapter_instance.client.chat.completions.create = MagicMock(return_value=iter(mock_chunks))
        adapter_instance.model_config.kwargs = {'stream': True}
        
        messages = [{"role": "user", "content": "Test streaming"}]
        
        # Capture print output to verify status messages
        with patch('builtins.print') as mock_print:
            result = adapter_instance._chat_completion_stream(messages)
        
        # Verify the client was called with correct parameters
        adapter_instance.client.chat.completions.create.assert_called_once()
        call_args = adapter_instance.client.chat.completions.create.call_args
        
        # Check that model and messages were passed correctly
        assert call_args[1]['model'] == adapter_instance.model_config.model_name
        assert call_args[1]['messages'] == messages
        assert call_args[1]['stream'] is True
        
        # Verify result contains the concatenated content
        assert result.choices[0].message.content == "chunk0chunk1chunk2"
        
        # Verify the response has the expected structure
        assert result.id == "test-id"
        assert result.model == adapter_instance.model_config.model_name
        assert result.choices[0].finish_reason == "stop"
        assert result.usage.prompt_tokens == 10
        assert result.usage.completion_tokens == 20
        assert result.usage.total_tokens == 30

    def test_streaming_config_attribute_access(self, adapter_instance):
        """Test that streaming config is properly accessed from model config."""
        # Test when stream attribute doesn't exist (should default to False)
        assert not hasattr(adapter_instance.model_config, 'stream')
        stream_enabled = getattr(adapter_instance.model_config, 'stream', False)
        assert stream_enabled is False
        
        # Test when stream attribute is explicitly set to True
        adapter_instance.model_config.stream = True
        stream_enabled = getattr(adapter_instance.model_config, 'stream', False)
        assert stream_enabled is True
        
        # Test when stream attribute is explicitly set to False
        adapter_instance.model_config.stream = False
        stream_enabled = getattr(adapter_instance.model_config, 'stream', False)
        assert stream_enabled is False
    
    def test_responses_streaming_enabled(self, adapter_instance):
        """Test that responses streaming is properly handled."""
        # Set up for responses API with streaming
        adapter_instance.model_config.api_type = APIType.RESPONSES
        adapter_instance.model_config.stream = True
        adapter_instance.model_config.kwargs = {'stream': True}
        
        # Mock the _responses_stream method
        with patch.object(adapter_instance, '_responses_stream') as mock_responses_stream:
            mock_response = MagicMock()
            mock_responses_stream.return_value = mock_response
            
            prompt = "Test responses streaming prompt"
            
            result = adapter_instance._call_ai_model(prompt)
            
            # Verify _responses_stream was called
            mock_responses_stream.assert_called_once()
            # Verify the result is our mock response
            assert result == mock_response
    
    def test_responses_streaming_with_background_raises_error(self, adapter_instance):
        """Test that enabling both streaming and background for responses API raises ValueError."""
        # Set up for responses API with both streaming and background enabled
        adapter_instance.model_config.api_type = APIType.RESPONSES
        adapter_instance.model_config.stream = True
        adapter_instance.model_config.background = True
        adapter_instance.model_config.kwargs = {'stream': True, 'background': True}
        
        prompt = "Test prompt"
        
        # Should raise ValueError when both streaming and background are enabled
        with pytest.raises(ValueError, match="Cannot enable both streaming and background for the responses API type"):
            adapter_instance._call_ai_model(prompt)
